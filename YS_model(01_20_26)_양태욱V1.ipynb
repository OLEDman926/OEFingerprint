{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OLEDman926/OEFingerprint/blob/main/YS_model(01_20_26)_%EC%96%91%ED%83%9C%EC%9A%B1V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Yejin Shim 고분자 응력필드 코드 (updated 01/20/2026)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#### 학습된 모델 셋업:\n",
        "\n",
        "*   epoch=50 (both global/local); lr=1e-3;\n",
        "*   dataset total=2400 (train=1920, val=240, test=240)\n",
        "*   batch=5\n",
        "*   save된 model weights directory: ./model/weights(01.20.26); output directory: ./model/out(01.20.26)"
      ],
      "metadata": {
        "id": "iwloxg8ScSro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Gdrive load"
      ],
      "metadata": {
        "id": "bTydmp_-Xu1d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAwrEa02XNOZ",
        "outputId": "0a42a03a-443b-44ff-d6a1-b07442cba2ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import csv\n",
        "import numpy as np\n",
        "import datetime\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import scipy\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "from keras import activations\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams.update({\n",
        "    \"font.family\": \"Liberation Sans\",\n",
        "    \"font.size\": 11,\n",
        "    \"axes.labelsize\": 11,\n",
        "    \"axes.titlesize\": 11,\n",
        "    \"xtick.labelsize\": 10,\n",
        "    \"ytick.labelsize\": 10,\n",
        "    \"legend.fontsize\": 10,\n",
        "    \"pdf.fonttype\": 42,\n",
        "    \"ps.fonttype\": 42,\n",
        "})\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "os.chdir(os.getcwd() + \"/drive/MyDrive/YS_고분자응력필드_dataset\") #---- \"YS_고분자응력필드_dataset\" 폴더 G-drive 위치로 설정해주세요"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1K6iZKiDcguk",
        "outputId": "2c9826eb-d792-489d-8280-5a77d0a9db8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/.shortcut-targets-by-id/1MI7GB2cSnswvIZvKi4PyjTyRmytr3OIQ/YS_고분자응력필드_dataset'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Preprocessing dataset"
      ],
      "metadata": {
        "id": "GfEOJW32Xz7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Keywords:\n",
        "\n",
        "*   geom: local/global 모델들 인풋\n",
        "*   target01: local 모델 아웃풋 groundtruth - [0,1]로 normalized된 target\n",
        "*   norm_stats: global 모델 아웃풋 - [min_log, max_log, eps] or [min_log, max_log]\n",
        "*   mm: [min_log and max_log]"
      ],
      "metadata": {
        "id": "tiCKLeDq6aWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# =========================================================================================\n",
        "# 1) Dataset 로드\n",
        "# geom (input), target01 (local_output), norm_stats (global_output)\n",
        "# ****norm_stats expected: [min_log, max_log, eps] or [min_log, max_log]\n",
        "# *eps(=1e-12): log로 처음 변환할 때 raw=0값이 negative infinity로 발산하지 않도록 더했음\n",
        "# =========================================================================================\n",
        "\n",
        "def load_one_geom_target_stats(stem: str, root=\".\", target_hw=None):\n",
        "    g_path = os.path.join(root, f\"{stem}-geom.npy\")\n",
        "    y_path = os.path.join(root, f\"{stem}-target.npy\")\n",
        "    s_path = os.path.join(root, f\"{stem}-norm_stats.npy\")\n",
        "\n",
        "    geom = np.load(g_path).astype(np.float32)      # (H,W)\n",
        "    target = np.load(y_path).astype(np.float32)    # (H,W)\n",
        "    stats = np.load(s_path).astype(np.float32)     # (2,) or (3,)\n",
        "\n",
        "    if geom.shape != target.shape:\n",
        "        raise ValueError(f\"shape mismatch: geom{geom.shape} vs target{target.shape} for stem={stem}\")\n",
        "\n",
        "    stats = np.array(stats, dtype=np.float32).reshape(-1)\n",
        "    if stats.size < 2:\n",
        "        raise ValueError(f\"norm_stats must have at least [min_log, max_log]; got {stats.shape}\")\n",
        "\n",
        "    if stats.size == 2:\n",
        "        stats = np.array([stats[0], stats[1], 1e-12], dtype=np.float32) # eps가 없을 경우에 default값으로 설정\n",
        "    else:\n",
        "        stats = stats[:3].astype(np.float32)\n",
        "\n",
        "    return geom, target, stats  # (H,W), (H,W), (3,)\n",
        "\n",
        "\n",
        "def load_dataset_geom_to_target(stems, root=\".\", target_hw=None):\n",
        "    X, Y, S = [], [], []\n",
        "\n",
        "    for stem in stems:\n",
        "        try:\n",
        "            g, y, s = load_one_geom_target_stats(stem, root=root, target_hw=target_hw)\n",
        "            X.append(g)\n",
        "            Y.append(y)\n",
        "            S.append(s)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipped {stem}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not X:\n",
        "        raise RuntimeError(\"No samples loaded. Check stems/root/patterns.\")\n",
        "\n",
        "    X = np.stack(X, axis=0).astype(np.float32)  # (N,H,W)\n",
        "    Y = np.stack(Y, axis=0).astype(np.float32)  # (N,H,W)\n",
        "    S = np.stack(S, axis=0).astype(np.float32)  # (N,3) = [min_log, max_log, eps]\n",
        "    return X, Y, S\n",
        "\n",
        "\n",
        "# ===========================================================================================\n",
        "# 2) 응력필드 reconstruction (inverse-normalize target01 using norm_stats [min_log, max_log, eps])\n",
        "# ===========================================================================================\n",
        "\n",
        "def reconstruct_stress_from_target(target01, norm_stats):\n",
        "    \"\"\"\n",
        "    (H,W) or (H,W,1): target01\n",
        "    (3,): norm_stats\n",
        "    \"\"\"\n",
        "    if target01.ndim == 3 and target01.shape[-1] == 1: #(H,W,1)인 경우에\n",
        "        target01 = target01[..., 0]\n",
        "\n",
        "    mm = np.array(norm_stats, dtype=np.float32).reshape(-1)\n",
        "    if mm.size < 2:\n",
        "        raise ValueError(f\"norm_stats must have [min_log,max_log,(eps)]; got {mm.shape}\")\n",
        "\n",
        "    min_log = float(mm[0])\n",
        "    max_log = float(mm[1])\n",
        "    eps = float(mm[2]) if mm.size >= 3 else 1e-12\n",
        "\n",
        "    log_field = target01.astype(np.float32) * (max_log - min_log) + min_log\n",
        "    stress = (10.0 ** log_field) - eps\n",
        "    return stress.astype(np.float32)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3) tf.data dataset 생성\n",
        "#\n",
        "#    ds_local: (geom, target01), ds_global (geom, mm)\n",
        "#    ds_full : (geom, target01, mm)\n",
        "# ============================================================\n",
        "\n",
        "def make_tf_dataset_local(X, Y, stats, batch=8, shuffle=4096):\n",
        "    X = X[..., None].astype(np.float32)  # (N,H,W,1)\n",
        "    Y = Y[..., None].astype(np.float32)  # (N,H,W,1)\n",
        "    stats = stats.astype(np.float32)     # (N,3)\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((X, Y, stats))\n",
        "    if shuffle and shuffle > 1:\n",
        "        ds = ds.shuffle(int(shuffle))\n",
        "    ds = ds.batch(int(batch)).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    ds_local = ds.map(lambda x, y, s: (x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return ds_local, ds\n",
        "\n",
        "\n",
        "def load_samples_local_plus_minmax(stems, root=\".\", target_hw=(128,128,1)):\n",
        "    \"\"\"\n",
        "    (N,H,W): geom\n",
        "    (N,H,W): target01\n",
        "    (N,2): mm\n",
        "    \"\"\"\n",
        "    geom, target01, s = load_dataset_geom_to_target(stems, root=root, target_hw=target_hw)\n",
        "    mm = s[:, :2].astype(np.float32)\n",
        "    return geom, target01, mm\n",
        "\n",
        "def make_tf_datasets(geom, target01, mm, batch=8, shuffle=4096):\n",
        "    \"\"\"\n",
        "    ds_full (x, target01, mm) 생성\n",
        "    \"\"\"\n",
        "    geom = geom[..., None].astype(np.float32)     # (N,H,W,1)\n",
        "    target01 = target01[..., None].astype(np.float32)  # (N,H,W,1)\n",
        "    mm = mm.astype(np.float32)              # (N,2)\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((geom, target01, mm))\n",
        "    if shuffle and shuffle > 1:\n",
        "        ds = ds.shuffle(int(shuffle))\n",
        "    ds = ds.batch(int(batch)).prefetch(tf.data.AUTOTUNE)\n",
        "    return ds #(geom, target01, mm)\n",
        "\n",
        "def split_two_model_datasets(ds_full):\n",
        "    \"\"\"\n",
        "    ds_local (x, target01)\n",
        "    ds_global (x, mm) 생성\n",
        "    \"\"\"\n",
        "    ds_local  = ds_full.map(lambda x, y, mm: (x, y),  num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds_global = ds_full.map(lambda x, y, mm: (x, mm), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return ds_local, ds_global #(geom, target01) and (geom, mm)\n",
        "\n",
        "\n",
        "# ========================\n",
        "# 6) Main loop functions\n",
        "# ========================\n",
        "\n",
        "def make_splits(n, train_frac=0.8, val_frac=0.1, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = np.arange(n)\n",
        "    rng.shuffle(idx)\n",
        "\n",
        "    n_train = int(n * train_frac)\n",
        "    n_val = int(n * val_frac)\n",
        "\n",
        "    train_ids = idx[:n_train]\n",
        "    val_ids   = idx[n_train:n_train + n_val]\n",
        "    test_ids  = idx[n_train + n_val:]\n",
        "    return train_ids, val_ids, test_ids\n",
        "\n",
        "def build_two_model_datasets(\n",
        "    all_ids,\n",
        "    root=\".\",\n",
        "    batch=8,\n",
        "    shuffle=4096,\n",
        "    seed=42,\n",
        "    target_hw=(128,128,1),\n",
        "):\n",
        "    # (1)Splitting the IDs\n",
        "    train_ids, val_ids, test_ids = make_splits(len(all_ids), seed=seed)\n",
        "    train_ids = [all_ids[i] for i in train_ids]\n",
        "    val_ids   = [all_ids[i] for i in val_ids]\n",
        "    test_ids  = [all_ids[i] for i in test_ids]\n",
        "\n",
        "    # (2)Load based on the IDs\n",
        "    geom_tr, target01_tr, mm_tr = load_samples_local_plus_minmax(train_ids, root=root, target_hw=target_hw)\n",
        "    geom_va, target01_va, mm_va = load_samples_local_plus_minmax(val_ids,   root=root, target_hw=target_hw)\n",
        "    geom_te, target01_te, mm_te = load_samples_local_plus_minmax(test_ids,  root=root, target_hw=target_hw)\n",
        "\n",
        "    # 3) tf.data datasets 생성\n",
        "    ds_tr = make_tf_datasets(geom_tr, target01_tr, mm_tr, batch=batch, shuffle=shuffle)\n",
        "    ds_va = make_tf_datasets(geom_va, target01_va, mm_va, batch=batch, shuffle=1)      # no shuffle\n",
        "    ds_te = make_tf_datasets(geom_te, target01_te, mm_te, batch=batch, shuffle=1)      # no shuffle\n",
        "\n",
        "    # 4) local_ds와 global_ds로 split\n",
        "    tr_local, tr_global = split_two_model_datasets(ds_tr)\n",
        "    va_local, va_global = split_two_model_datasets(ds_va)\n",
        "    te_local, te_global = split_two_model_datasets(ds_te)\n",
        "\n",
        "    return {\n",
        "        \"ids\": {\"train\": train_ids, \"val\": val_ids, \"test\": test_ids},\n",
        "        \"local\":  {\"train\": tr_local,  \"val\": va_local,  \"test\": te_local},\n",
        "        \"global\": {\"train\": tr_global, \"val\": va_global, \"test\": te_global},\n",
        "        \"shapes\": {\"X\": geom_tr.shape[1:], \"Y_local\": target01_tr.shape[1:], \"minmax\": mm_tr.shape[1:]},\n",
        "    }\n"
      ],
      "metadata": {
        "id": "1sPsxk68XmZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Model build & training"
      ],
      "metadata": {
        "id": "aaJFWMbuX4EW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# ======================\n",
        "# 1) Model architectures\n",
        "# ======================\n",
        "\n",
        "def _conv_block(x, filters, name):\n",
        "  x = tf.keras.layers.Conv2D(filters, 3, padding=\"same\", activation=\"relu\", name=f\"{name}_c1\")(x)\n",
        "  x = tf.keras.layers.Conv2D(filters, 3, padding=\"same\", activation=\"relu\", name=f\"{name}_c2\")(x)\n",
        "  return x\n",
        "\n",
        "def _match_hw(x, ref, name):\n",
        "  return tf.keras.layers.Lambda(\n",
        "      lambda t: tf.image.resize(t[0], tf.shape(t[1])[1:3], method=\"bilinear\"),\n",
        "      name=name\n",
        "  )([x, ref])\n",
        "\n",
        "def build_local_unet(input_shape, base=16):\n",
        "  inp = tf.keras.Input(shape=input_shape, name=\"geom_in\")\n",
        "\n",
        "  e1 = _conv_block(inp, base,   \"enc1\"); p1 = tf.keras.layers.MaxPool2D(2)(e1)\n",
        "  e2 = _conv_block(p1,  base*2,   \"enc2\"); p2 = tf.keras.layers.MaxPool2D(2)(e2)\n",
        "  e3 = _conv_block(p2,  base*4,   \"enc3\"); p3 = tf.keras.layers.MaxPool2D(2)(e3)\n",
        "  e4 = _conv_block(p3,  base*8,   \"enc4\"); p4 = tf.keras.layers.MaxPool2D(2)(e4)\n",
        "\n",
        "  b  = _conv_block(p4,  base*16, \"bottleneck\")\n",
        "\n",
        "  u4 = tf.keras.layers.UpSampling2D(2)(b);  u4 = _match_hw(u4, e4, \"m4\")\n",
        "  d4 = _conv_block(tf.keras.layers.Concatenate()([u4, e4]), base*8, \"dec4\")\n",
        "\n",
        "  u3 = tf.keras.layers.UpSampling2D(2)(d4); u3 = _match_hw(u3, e3, \"m3\")\n",
        "  d3 = _conv_block(tf.keras.layers.Concatenate()([u3, e3]), base*4, \"dec3\")\n",
        "\n",
        "  u2 = tf.keras.layers.UpSampling2D(2)(d3); u2 = _match_hw(u2, e2, \"m2\")\n",
        "  d2 = _conv_block(tf.keras.layers.Concatenate()([u2, e2]), base*2, \"dec2\")\n",
        "\n",
        "  u1 = tf.keras.layers.UpSampling2D(2)(d2); u1 = _match_hw(u1, e1, \"m1\")\n",
        "  d1 = _conv_block(tf.keras.layers.Concatenate()([u1, e1]), base, \"dec1\")\n",
        "\n",
        "  out = tf.keras.layers.Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\", name=\"y_log_01\")(d1)\n",
        "  return tf.keras.Model(inp, out, name=\"Local-U-Net\")\n",
        "\n",
        "\n",
        "def build_global_minmax_model(input_shape, base=16, dropout=0.0):\n",
        "  inp = tf.keras.Input(shape=input_shape, name=\"geom_in\")\n",
        "\n",
        "  x = tf.keras.layers.Conv2D(base,   3, padding=\"same\", activation=\"relu\")(inp)\n",
        "  x = tf.keras.layers.Conv2D(base,   3, padding=\"same\", activation=\"relu\")(x)\n",
        "  x = tf.keras.layers.MaxPool2D(2)(x)\n",
        "\n",
        "  x = tf.keras.layers.Conv2D(base*2, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "  x = tf.keras.layers.Conv2D(base*2, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "  x = tf.keras.layers.MaxPool2D(2)(x)\n",
        "\n",
        "  x = tf.keras.layers.Conv2D(base*4, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "  x = tf.keras.layers.Conv2D(base*4, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "  x = tf.keras.layers.MaxPool2D(2)(x)\n",
        "\n",
        "  x = tf.keras.layers.Conv2D(base*8, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "  x = tf.keras.layers.Conv2D(base*8, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "\n",
        "  gap = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "  gmp = tf.keras.layers.GlobalMaxPooling2D()(x)\n",
        "  g = tf.keras.layers.Concatenate()([gap, gmp])\n",
        "\n",
        "  g = tf.keras.layers.Dense(128, activation=\"relu\")(g)\n",
        "  if dropout and dropout > 0:\n",
        "    g = tf.keras.layers.Dropout(dropout)(g)\n",
        "  g = tf.keras.layers.Dense(64, activation=\"relu\")(g)\n",
        "\n",
        "  min_log = tf.keras.layers.Dense(1, activation=\"linear\", name=\"min_log\")(g)\n",
        "  raw_range = tf.keras.layers.Dense(1, activation=\"linear\", name=\"raw_range\")(g)\n",
        "  range_log = tf.keras.layers.Activation(tf.nn.softplus, name=\"range_log\")(raw_range)\n",
        "  max_log = tf.keras.layers.Add(name=\"max_log\")([min_log, range_log])\n",
        "\n",
        "  out = tf.keras.layers.Concatenate(name=\"minmax\")([min_log, max_log])  # (B,2)\n",
        "  return tf.keras.Model(inp, out, name=\"Global-MinMax\")\n",
        "\n",
        "\n",
        "# ==================\n",
        "# 2) MSE loss function\n",
        "# ==================\n",
        "@tf.function\n",
        "def plain_mse(y_true, y_pred):\n",
        "  return tf.reduce_mean(tf.square(y_true - y_pred))\n",
        "\n",
        "\n",
        "# ===============\n",
        "# 3) Local trainer\n",
        "# ===============\n",
        "\n",
        "def train_local_unet(\n",
        "    ds_train, ds_val, ds_test, input_shape,\n",
        "    epochs=15, lr=1e-3,\n",
        "    weights_dir=\"./model/weights/local\"\n",
        "):\n",
        "  os.makedirs(weights_dir, exist_ok=True)\n",
        "\n",
        "  model = build_local_unet(input_shape=input_shape, base=16)\n",
        "  opt = tf.keras.optimizers.Adam(lr)\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "      pred = model(x, training=True)\n",
        "      loss = plain_mse(y, pred)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    opt.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "  @tf.function\n",
        "  def eval_step(x, y):\n",
        "    pred = model(x, training=False)\n",
        "    return plain_mse(y, pred)\n",
        "\n",
        "  for ep in range(1, epochs + 1):\n",
        "    tr = [float(train_step(x, y).numpy()) for x, y in ds_train]\n",
        "    va = [float(eval_step(x, y).numpy())  for x, y in ds_val]\n",
        "    print(f\"[Local-U-Net] Ep {ep:02d} | train={np.mean(tr):.6f} | val={np.mean(va):.6f}\")\n",
        "\n",
        "    wpath = os.path.join(weights_dir, f\"local_unet_ep{ep:04d}.weights.h5\")\n",
        "    model.save_weights(wpath)\n",
        "\n",
        "  te = [float(eval_step(x, y).numpy()) for x, y in ds_test]\n",
        "  print(f\"[Local-U-Net] TEST={np.mean(te):.6f}\")\n",
        "  return model\n",
        "\n",
        "\n",
        "# ===============\n",
        "# 4) Global trainer\n",
        "# ===============\n",
        "\n",
        "class _SaveGlobalWeightsEachEpoch(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, weights_dir=\"./model/weights/global\"):\n",
        "    super().__init__()\n",
        "    self.weights_dir = weights_dir\n",
        "    os.makedirs(self.weights_dir, exist_ok=True)\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    ep = int(epoch) + 1\n",
        "    wpath = os.path.join(self.weights_dir, f\"global_minmax_ep{ep:04d}.weights.h5\")\n",
        "    self.model.save_weights(wpath)\n",
        "\n",
        "\n",
        "def train_global_minmax(\n",
        "    ds_train, ds_val, ds_test, input_shape,\n",
        "    epochs=15, lr=1e-3,\n",
        "    weights_dir=\"./model/weights/global\"\n",
        "):\n",
        "  os.makedirs(weights_dir, exist_ok=True)\n",
        "\n",
        "  model = build_global_minmax_model(input_shape=input_shape, base=16, dropout=0.0)\n",
        "  model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(lr),\n",
        "    loss=tf.keras.losses.MeanSquaredError(),\n",
        "    metrics=[tf.keras.metrics.MeanAbsoluteError(name=\"mae\")]\n",
        "  )\n",
        "\n",
        "  cb = _SaveGlobalWeightsEachEpoch(weights_dir=weights_dir)\n",
        "\n",
        "  model.fit(\n",
        "    ds_train,\n",
        "    validation_data=ds_val,\n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    callbacks=[cb]\n",
        "  )\n",
        "\n",
        "  model.evaluate(ds_test, verbose=1)\n",
        "  return model\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# NOT currently using this\n",
        "# ==========================\n",
        "\n",
        "def train_local_and_global_from_bundles(\n",
        "    bundles,\n",
        "    epochs_local=15,\n",
        "    epochs_global=15,\n",
        "    lr=1e-3,\n",
        "    weights_dir=\"./model/weights\",\n",
        "):\n",
        "\n",
        "  os.makedirs(weights_dir, exist_ok=True)\n",
        "\n",
        "  ds_local_train = bundles[\"local\"][\"train\"]\n",
        "  ds_local_val   = bundles[\"local\"][\"val\"]\n",
        "  ds_local_test  = bundles[\"local\"][\"test\"]\n",
        "\n",
        "  ds_global_train = bundles[\"global\"][\"train\"]\n",
        "  ds_global_val   = bundles[\"global\"][\"val\"]\n",
        "  ds_global_test  = bundles[\"global\"][\"test\"]\n",
        "\n",
        "  geom_0, _ = next(iter(ds_local_train)) # input_shape이 model build할 때 필요해서\n",
        "  input_shape = tuple(geom_0.shape[1:])  # (H,W,1)\n",
        "\n",
        "  local_unet = train_local_unet(\n",
        "    ds_local_train, ds_local_val, ds_local_test,\n",
        "    input_shape=input_shape,\n",
        "    epochs=epochs_local, lr=lr,\n",
        "    weights_dir=weights_dir\n",
        "  )\n",
        "\n",
        "  global_model = train_global_minmax(\n",
        "    ds_global_train, ds_global_val, ds_global_test,\n",
        "    input_shape=input_shape,\n",
        "    epochs=epochs_global, lr=lr,\n",
        "    weights_dir=weights_dir\n",
        "  )\n",
        "\n",
        "  return local_unet, global_model"
      ],
      "metadata": {
        "id": "BnTnSPgbX7r6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "\n",
        "# ============================================================\n",
        "# 0) Dataset 스캔: getting STEMS from files on disk\n",
        "#    파일 포맷:\n",
        "#      <stem>-geom.npy\n",
        "#      <stem>-target.npy\n",
        "#      <stem>-norm_stats.npy\n",
        "# ============================================================\n",
        "\n",
        "def list_dataset_stems(root=\".\"):\n",
        "    root = Path(root)\n",
        "\n",
        "    def stems_with_suffix(suffix: str):\n",
        "        # <stem><suffix>로 포맷된 stem files return\n",
        "        return {p.name[:-len(suffix)] for p in root.glob(f\"*{suffix}\")}\n",
        "\n",
        "    geom = stems_with_suffix(\"-geom.npy\")\n",
        "    targ = stems_with_suffix(\"-target.npy\")\n",
        "    stat = stems_with_suffix(\"-norm_stats.npy\")\n",
        "\n",
        "    stems = sorted(geom & targ & stat)\n",
        "\n",
        "    if not stems:\n",
        "        print(f\"[DEBUG] folder: {root.resolve()}\")\n",
        "        print(f\"[DEBUG] counts: geom={len(geom)} target={len(targ)} norm_stats={len(stat)}\")\n",
        "        examples = sorted([p.name for p in root.glob(\"*.npy\")])[:25]\n",
        "        print(f\"[DEBUG] example files (first 25): {examples}\")\n",
        "        raise RuntimeError(\n",
        "            f\"No valid samples found in {root.resolve()}. \"\n",
        "            \"Need <stem>-geom.npy + <stem>-target.npy + <stem>-norm_stats.npy.\"\n",
        "        )\n",
        "\n",
        "    return stems\n",
        "\n",
        "# ============================================================\n",
        "# 1) Stem에서 데이터 로드: (geom, target01, mm)\n",
        "# ============================================================\n",
        "\n",
        "def load_samples_geom_target_minmax( # X=geom; Y=target01; MM=[min_log, max_log]\n",
        "    stems,\n",
        "    root=\".\",\n",
        "    geom_pat=\"{}-geom.npy\",\n",
        "    target_pat=\"{}-target.npy\",\n",
        "    minmax_pat=\"{}-norm_stats.npy\",\n",
        "):\n",
        "    X, Y, MM = [], [], []\n",
        "\n",
        "    for stem in stems:\n",
        "        try:\n",
        "            x = np.load(os.path.join(root, geom_pat.format(stem))).astype(np.float32)    # (H,W)\n",
        "            y = np.load(os.path.join(root, target_pat.format(stem))).astype(np.float32) # (H,W)\n",
        "            mm = np.load(os.path.join(root, minmax_pat.format(stem))).astype(np.float32)\n",
        "\n",
        "            if x.shape != y.shape:\n",
        "                raise ValueError(f\"shape mismatch: geom{x.shape} vs target{y.shape}\")\n",
        "\n",
        "            mm = np.array(mm, dtype=np.float32).reshape(-1)\n",
        "            if mm.size < 2:\n",
        "                raise ValueError(f\"norm_stats must start with [min_log,max_log]; got shape {mm.shape}\")\n",
        "\n",
        "            X.append(x)\n",
        "            Y.append(y)\n",
        "            MM.append(mm[:2])  # keep only [min_log, max_log] (eps 제외)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Skipped {stem}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not X:\n",
        "        raise RuntimeError(\"No samples loaded after filtering/skips.\")\n",
        "\n",
        "    X = np.stack(X, axis=0).astype(np.float32)      # (N,H,W)\n",
        "    Y = np.stack(Y, axis=0).astype(np.float32)      # (N,H,W)\n",
        "    MM = np.stack(MM, axis=0).astype(np.float32)    # (N,2)\n",
        "    return X, Y, MM\n",
        "\n",
        "# ============================================================\n",
        "# 2) tf.data.Dataset 생성 후 local/global dataset으로 각각 split\n",
        "# ============================================================\n",
        "\n",
        "def make_tf_dataset_full(X, Y, MM, batch=8, shuffle=4096): # X=geom; Y=target01; MM=[min_log, max_log]\n",
        "    X = X[..., None].astype(np.float32)   # (N,H,W,1)\n",
        "    Y = Y[..., None].astype(np.float32)   # (N,H,W,1)\n",
        "    MM = MM.astype(np.float32)            # (N,2)\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((X, Y, MM))\n",
        "\n",
        "    if shuffle and int(shuffle) > 1:\n",
        "        buf = min(int(shuffle), int(X.shape[0])) # shuffle buffer size는 dataset size보다 작아야함\n",
        "        ds = ds.shuffle(buf)\n",
        "\n",
        "    ds = ds.batch(int(batch)).prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "def split_local_global(ds_full):\n",
        "    ds_local  = ds_full.map(lambda x, y, mm: (x, y),  num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds_global = ds_full.map(lambda x, y, mm: (x, mm), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return ds_local, ds_global\n",
        "\n",
        "# ============================================================\n",
        "# 3) Train/val/test split\n",
        "# ============================================================\n",
        "\n",
        "def split_stems(stems, train_frac=0.8, val_frac=0.1, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    stems = list(stems)\n",
        "    rng.shuffle(stems)\n",
        "\n",
        "    n = len(stems)\n",
        "    n_train = int(n * train_frac)\n",
        "    n_val = int(n * val_frac)\n",
        "\n",
        "    train = stems[:n_train]\n",
        "    val   = stems[n_train:n_train + n_val]\n",
        "    test  = stems[n_train + n_val:]\n",
        "    return train, val, test\n",
        "\n",
        "# ============================================================\n",
        "# 4) Main train pipeline\n",
        "# ============================================================\n",
        "\n",
        "def main_train_pipeline( # X=geom; Y=target01; MM=[min_log, max_log]\n",
        "    data_root=\".\",\n",
        "    batch=8,\n",
        "    shuffle=4096,\n",
        "    seed=42,\n",
        "    epochs_local=15,\n",
        "    epochs_global=15,\n",
        "    lr=1e-3,\n",
        "):\n",
        "    #-(1) root directory에서 dataset retrieve\n",
        "    stems_all = list_dataset_stems(data_root)\n",
        "    print(f\"Found {len(stems_all)} valid samples under: {os.path.abspath(data_root)}\")\n",
        "\n",
        "    #-(2) split\n",
        "    stems_tr, stems_va, stems_te = split_stems(stems_all, seed=seed)\n",
        "    print(f\"Split: train={len(stems_tr)}, val={len(stems_va)}, test={len(stems_te)}\")\n",
        "\n",
        "    #-(3) arrays 로드\n",
        "    X_tr, Y_tr, MM_tr = load_samples_geom_target_minmax(stems_tr, root=data_root)\n",
        "    X_va, Y_va, MM_va = load_samples_geom_target_minmax(stems_va, root=data_root)\n",
        "    X_te, Y_te, MM_te = load_samples_geom_target_minmax(stems_te, root=data_root)\n",
        "\n",
        "    #-(4) tf.data.Dataset 생성\n",
        "    ds_tr_full = make_tf_dataset_full(X_tr, Y_tr, MM_tr, batch=batch, shuffle=shuffle)\n",
        "    ds_va_full = make_tf_dataset_full(X_va, Y_va, MM_va, batch=batch, shuffle=1)\n",
        "    ds_te_full = make_tf_dataset_full(X_te, Y_te, MM_te, batch=batch, shuffle=1)\n",
        "\n",
        "    ds_local_tr, ds_global_tr = split_local_global(ds_tr_full)\n",
        "    ds_local_va, ds_global_va = split_local_global(ds_va_full)\n",
        "    ds_local_te, ds_global_te = split_local_global(ds_te_full)\n",
        "\n",
        "    #-(5) input_shape 확보 (model build할 때 필요)\n",
        "    geom0, _ = next(iter(ds_local_tr))          # geom0: (B,H,W,1)\n",
        "    input_shape = tuple(geom0.shape[1:])        # (H,W,1)\n",
        "    print(\"Input shape:\", input_shape)\n",
        "\n",
        "    #-(6) Local U-Net 모델 학습\n",
        "    local_unet = train_local_unet(\n",
        "        ds_local_tr, ds_local_va, ds_local_te,\n",
        "        input_shape=input_shape,\n",
        "        epochs=epochs_local,\n",
        "        lr=lr,\n",
        "        weights_dir=\"./model/weights/local\" #<-- specify the local weights path here\n",
        "    )\n",
        "    #-(7) Global min/max 모델 학습\n",
        "    global_model = train_global_minmax(\n",
        "        ds_global_tr, ds_global_va, ds_global_te,\n",
        "        input_shape=input_shape,\n",
        "        epochs=epochs_global,\n",
        "        lr=lr,\n",
        "        weights_dir=\"./model/weights/global\" #<-- specify the global weights path here\n",
        "    )\n",
        "\n",
        "    bundles = {\n",
        "        \"ids\": {\"train\": stems_tr, \"val\": stems_va, \"test\": stems_te},\n",
        "        \"local\":  {\"train\": ds_local_tr,  \"val\": ds_local_va,  \"test\": ds_local_te},\n",
        "        \"global\": {\"train\": ds_global_tr, \"val\": ds_global_va, \"test\": ds_global_te},\n",
        "    }\n",
        "\n",
        "    return local_unet, global_model, bundles\n",
        "\n",
        "\n",
        "def build_bundles_only(\n",
        "    data_root=\".\",\n",
        "    batch=8,\n",
        "    shuffle=4096,\n",
        "    seed=42,\n",
        "    train_frac=0.8,\n",
        "    val_frac=0.1,\n",
        "):\n",
        "    # (1) stem files retrieve\n",
        "    stems_all = list_dataset_stems(data_root)\n",
        "    print(f\"Found {len(stems_all)} valid samples under: {os.path.abspath(data_root)}\")\n",
        "\n",
        "    # (2) split\n",
        "    stems_tr, stems_va, stems_te = split_stems(\n",
        "        stems_all, train_frac=train_frac, val_frac=val_frac, seed=seed\n",
        "    )\n",
        "    print(f\"Split: train={len(stems_tr)}, val={len(stems_va)}, test={len(stems_te)}\")\n",
        "\n",
        "    # (3) arrays 로드\n",
        "    X_tr, Y_tr, MM_tr = load_samples_geom_target_minmax(stems_tr, root=data_root)\n",
        "    X_va, Y_va, MM_va = load_samples_geom_target_minmax(stems_va, root=data_root)\n",
        "    X_te, Y_te, MM_te = load_samples_geom_target_minmax(stems_te, root=data_root)\n",
        "\n",
        "    # (4) tf.data datasets 생성 / split\n",
        "    ds_tr_full = make_tf_dataset_full(X_tr, Y_tr, MM_tr, batch=batch, shuffle=shuffle)\n",
        "    ds_va_full = make_tf_dataset_full(X_va, Y_va, MM_va, batch=batch, shuffle=1)\n",
        "    ds_te_full = make_tf_dataset_full(X_te, Y_te, MM_te, batch=batch, shuffle=1)\n",
        "\n",
        "    ds_local_tr, ds_global_tr = split_local_global(ds_tr_full)\n",
        "    ds_local_va, ds_global_va = split_local_global(ds_va_full)\n",
        "    ds_local_te, ds_global_te = split_local_global(ds_te_full)\n",
        "\n",
        "    # (5) input_shape 확보\n",
        "    x0, y0 = next(iter(ds_local_tr))      # x0: (B,H,W,1)\n",
        "    input_shape = tuple(x0.shape[1:])     # (H,W,1)\n",
        "    print(\"Input shape:\", input_shape)\n",
        "\n",
        "    bundles = {\n",
        "        \"ids\": {\"train\": stems_tr, \"val\": stems_va, \"test\": stems_te},\n",
        "        \"local\":  {\"train\": ds_local_tr,  \"val\": ds_local_va,  \"test\": ds_local_te},\n",
        "        \"global\": {\"train\": ds_global_tr, \"val\": ds_global_va, \"test\": ds_global_te},\n",
        "    }\n",
        "    return bundles, input_shape"
      ],
      "metadata": {
        "id": "e2eY2a1hYHLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Main"
      ],
      "metadata": {
        "id": "f_wE1I_eYNqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 5) Entry point\n",
        "# ============================================================\n",
        "if __name__ == \"__main__\":\n",
        "    local_unet, global_model, bundles = main_train_pipeline(  #각 epoch마다 weights.h5 save\n",
        "        data_root=\"./dataset/combined\",\n",
        "        batch=5,\n",
        "        shuffle=6000,\n",
        "        seed=123,\n",
        "        epochs_local=1,\n",
        "        epochs_global=1,\n",
        "        lr=1e-3,\n",
        "    )"
      ],
      "metadata": {
        "id": "S9A5SocvhUL2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e20893cc-3e94-4427-a209-617fc3b69adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2400 valid samples under: /content/drive/.shortcut-targets-by-id/1MI7GB2cSnswvIZvKi4PyjTyRmytr3OIQ/YS_고분자응력필드_dataset/dataset/combined\n",
            "Split: train=1920, val=240, test=240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Model test\n",
        "###*Must run the above helper functions (except the main)"
      ],
      "metadata": {
        "id": "KnoBxoMNYS1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
        "\n",
        "# ============================================================\n",
        "# (1) Dataset 유틸리티 함수들\n",
        "# ============================================================\n",
        "\n",
        "def list_dataset_stems(root=\".\"):\n",
        "    stems = []\n",
        "    for gp in sorted(glob.glob(os.path.join(root, \"*-geom.npy\"))):\n",
        "        stem = os.path.basename(gp)[:-len(\"-geom.npy\")]\n",
        "        if (os.path.exists(os.path.join(root, f\"{stem}-target.npy\")) and\n",
        "            os.path.exists(os.path.join(root, f\"{stem}-norm_stats.npy\"))):\n",
        "            stems.append(stem)\n",
        "    if not stems:\n",
        "        raise RuntimeError(f\"No valid stems found under: {os.path.abspath(root)}\")\n",
        "    return stems\n",
        "\n",
        "\n",
        "def _stats_to_minmax_eps(norm_stats, default_eps=1e-12):\n",
        "    s = np.array(norm_stats, dtype=np.float32).reshape(-1)\n",
        "    if s.size < 2:\n",
        "        raise ValueError(\"norm_stats must contain at least [min_log, max_log]\")\n",
        "    min_log = float(s[0])\n",
        "    max_log = float(s[1])\n",
        "    eps = float(s[2]) if s.size >= 3 else float(default_eps)\n",
        "    return min_log, max_log, eps\n",
        "\n",
        "\n",
        "def load_one(stem, root=\".\"):\n",
        "    g = np.load(os.path.join(root, f\"{stem}-geom.npy\")).astype(np.float32)\n",
        "    y = np.load(os.path.join(root, f\"{stem}-target.npy\")).astype(np.float32)\n",
        "    s = np.load(os.path.join(root, f\"{stem}-norm_stats.npy\")).astype(np.float32)\n",
        "\n",
        "    if g.shape != y.shape:\n",
        "        raise ValueError(f\"shape mismatch: geom{g.shape} vs target{y.shape}\")\n",
        "\n",
        "    return g, y, s\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# (2) 예측한 응력필드 reconstruct / evaluate\n",
        "# ============================================================\n",
        "\n",
        "def recon_log_field(local01_hw, minmax):\n",
        "    if local01_hw.ndim == 3 and local01_hw.shape[-1] == 1:\n",
        "        local01_hw = local01_hw[..., 0]\n",
        "    min_log, max_log = float(minmax[0]), float(minmax[1])\n",
        "    return local01_hw.astype(np.float32) * (max_log - min_log) + min_log\n",
        "\n",
        "\n",
        "def eval_one(local_unet, global_model, geom_hw):\n",
        "    x = geom_hw[None, ..., None].astype(np.float32)  # (1,H,W,1)\n",
        "\n",
        "    y_pred = local_unet(x, training=False).numpy()[0]\n",
        "    y_pred01 = y_pred[..., 0] if y_pred.ndim == 3 else np.asarray(y_pred).squeeze()\n",
        "\n",
        "    mm_pred = global_model(x, training=False).numpy()[0]\n",
        "    mm_pred = np.asarray(mm_pred, dtype=np.float32).reshape(-1)\n",
        "\n",
        "    if mm_pred.size < 2:\n",
        "        raise ValueError(\"global_model must output [min_log, max_log]\")\n",
        "\n",
        "    return y_pred01.astype(np.float32), mm_pred[:2].astype(np.float32)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# (3) Metrics (testing)\n",
        "# ============================================================\n",
        "\n",
        "def compute_test_metrics(local_unet, global_model, stems, root=\".\", max_items=None):\n",
        "    local_mse_list = []\n",
        "    global_mse_list = []\n",
        "    n_done = 0\n",
        "\n",
        "    for stem in stems:\n",
        "        if max_items is not None and n_done >= int(max_items):\n",
        "            break\n",
        "        try:\n",
        "            geom, y_true01, stats = load_one(stem, root=root)\n",
        "            min_log, max_log, _ = _stats_to_minmax_eps(stats)\n",
        "\n",
        "            y_pred01, mm_pred = eval_one(local_unet, global_model, geom)\n",
        "\n",
        "            local_mse = float(np.mean((y_pred01 - y_true01) ** 2))\n",
        "            mm_true = np.array([min_log, max_log], dtype=np.float32)\n",
        "            global_mse = float(np.mean((mm_pred - mm_true) ** 2))\n",
        "\n",
        "            print(f\"stem:{stem}\")\n",
        "            print(f\"mm_pred:{mm_pred}, mm_true:{mm_true}\")\n",
        "\n",
        "            local_mse_list.append(local_mse)\n",
        "            global_mse_list.append(global_mse)\n",
        "            n_done += 1\n",
        "\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    if n_done == 0:\n",
        "        raise RuntimeError(\"No samples evaluated.\")\n",
        "\n",
        "    return {\n",
        "        \"n\": n_done,\n",
        "        \"local_mse_mean\": float(np.mean(local_mse_list)),\n",
        "        \"local_mse_std\": float(np.std(local_mse_list)),\n",
        "        \"global_mse_mean\": float(np.mean(global_mse_list)),\n",
        "        \"global_mse_std\": float(np.std(global_mse_list)),\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# (4) Visualization 함수들\n",
        "# ============================================================\n",
        "\n",
        "def plot_geom_local_recon(stem, geom, y_true01, y_pred01, minmax_true, minmax_pred, savepath=None):\n",
        "\n",
        "    log_true = recon_log_field(y_true01, minmax_true)\n",
        "    log_pred = recon_log_field(y_pred01, minmax_pred)\n",
        "\n",
        "    #-global color limits\n",
        "    vminL = min(log_true.min(), log_pred.min())\n",
        "    vmaxL = max(log_true.max(), log_pred.max())\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 9))\n",
        "    gs = fig.add_gridspec(3, 2, hspace=0.25, wspace=0.15)\n",
        "\n",
        "    ax0 = fig.add_subplot(gs[0, :])\n",
        "    #cmap = ListedColormap([\"blue\", \"red\"])\n",
        "    #bounds = [-0.01, 0.5, 1.01]\n",
        "    #norm = BoundaryNorm(bounds, cmap.N)\n",
        "\n",
        "    im0 = ax0.imshow(geom, cmap='binary', origin=\"upper\")\n",
        "    #im0 = ax0.imshow(geom, cmap=cmap, norm=norm, interpolation=\"nearest\")\n",
        "    ax0.set_title(\"Geometry (input)\")\n",
        "    ax0.axis(\"off\")\n",
        "    plt.colorbar(im0, ax=ax0, fraction=0.02, pad=0.01)\n",
        "\n",
        "    ax1 = fig.add_subplot(gs[1, 0])\n",
        "    im1 = ax1.imshow(y_pred01, vmin=0, vmax=1, origin=\"upper\")\n",
        "    ax1.set_title(\"Normalized log-stress (prediction)\")\n",
        "    ax1.axis(\"off\")\n",
        "    plt.colorbar(im1, ax=ax1, fraction=0.046, pad=0.02)\n",
        "\n",
        "    ax2 = fig.add_subplot(gs[1, 1])\n",
        "    im2 = ax2.imshow(y_true01, vmin=0, vmax=1, origin=\"upper\")\n",
        "    ax2.set_title(\"Normalized log-stress (groundtruth)\")\n",
        "    ax2.axis(\"off\")\n",
        "    plt.colorbar(im2, ax=ax2, fraction=0.046, pad=0.02)\n",
        "\n",
        "    ax3 = fig.add_subplot(gs[2, 0])\n",
        "    im3 = ax3.imshow(log_pred, vmin=vminL, vmax=vmaxL, origin=\"upper\")\n",
        "    ax3.set_title(\"Reconstructed log-stress (prediction)\")\n",
        "    ax3.axis(\"off\")\n",
        "    plt.colorbar(im3, ax=ax3, fraction=0.046, pad=0.02)\n",
        "\n",
        "    ax4 = fig.add_subplot(gs[2, 1])\n",
        "    im4 = ax4.imshow(log_true, vmin=vminL, vmax=vmaxL, origin=\"upper\")\n",
        "    ax4.set_title(\"Reconstructed log-stress (groundtruth)\")\n",
        "    ax4.axis(\"off\")\n",
        "    plt.colorbar(im4, ax=ax4, fraction=0.046, pad=0.02)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if savepath is not None:\n",
        "        os.makedirs(savepath, exist_ok=True)\n",
        "        plt.savefig(os.path.join(savepath, f\"{stem}.png\"), dpi=180)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def visualize_examples(local_unet, global_model, stems, root=\".\", n=5, seed=0, figpath=None):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    stems = list(stems)\n",
        "    rng.shuffle(stems)\n",
        "\n",
        "    shown = 0\n",
        "    for stem in stems:\n",
        "        try:\n",
        "            geom, y_true01, stats = load_one(stem, root=root)\n",
        "            min_log, max_log, _ = _stats_to_minmax_eps(stats)\n",
        "\n",
        "            y_pred01, mm_pred = eval_one(local_unet, global_model, geom)\n",
        "\n",
        "            plot_geom_local_recon(\n",
        "                stem=stem,\n",
        "                geom=geom,\n",
        "                y_true01=y_true01,\n",
        "                y_pred01=y_pred01,\n",
        "                minmax_true=[min_log, max_log],\n",
        "                minmax_pred=mm_pred,\n",
        "                savepath=figpath\n",
        "            )\n",
        "\n",
        "            shown += 1\n",
        "            if shown >= n:\n",
        "                break\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    if shown == 0:\n",
        "        raise RuntimeError(\"No examples visualized.\")\n",
        "    if shown < n:\n",
        "        print(f\"Only displayed {shown} examples (requested {n}).\")"
      ],
      "metadata": {
        "id": "HwZ-o4tjYUmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================================\n",
        "# 1) Main test pipeline\n",
        "# load models if paths are given - otherwise uses the as-trained model\n",
        "# ====================================================================\n",
        "\n",
        "def main_test_only(\n",
        "    data_root=\".\",\n",
        "    local_model_path=None,\n",
        "    global_model_path=None,\n",
        "    stems_test=None,\n",
        "    input_shape=(128,128,1),\n",
        "    n_vis=10,\n",
        "    seed=1,\n",
        "    do_visualize=True,\n",
        "    do_metrics=True,\n",
        "    max_items_for_metrics=None,\n",
        "    figpath=None\n",
        "):\n",
        "    # 1) stems\n",
        "    if stems_test is None:\n",
        "        stems_test = list_dataset_stems(data_root)\n",
        "\n",
        "    # 2) models (load if paths provided)\n",
        "    local_model = None\n",
        "    global_mm_model = None\n",
        "\n",
        "    if local_model_path is not None:\n",
        "        local_model = build_local_unet(input_shape=input_shape)\n",
        "        local_model.load_weights(local_model_path)\n",
        "    if global_model_path is not None:\n",
        "        global_mm_model = build_global_minmax_model(input_shape=input_shape)\n",
        "        global_mm_model.load_weights(global_model_path)\n",
        "\n",
        "    # 3) metrics\n",
        "    results = None\n",
        "    if do_metrics:\n",
        "        results = compute_test_metrics(\n",
        "            local_model,\n",
        "            global_mm_model,\n",
        "            stems_test,\n",
        "            root=data_root,\n",
        "            max_items=max_items_for_metrics\n",
        "        )\n",
        "        print(\n",
        "            f\"[TEST METRICS] n={results['n']} | \"\n",
        "            f\"local_mse={results['local_mse_mean']:.6g} ± {results['local_mse_std']:.6g} | \"\n",
        "            f\"global_mse={results['global_mse_mean']:.6g} ± {results['global_mse_std']:.6g}\"\n",
        "        )\n",
        "\n",
        "    # 4) visualization\n",
        "    if do_visualize:\n",
        "        visualize_examples(\n",
        "            local_model,\n",
        "            global_mm_model,\n",
        "            stems_test,\n",
        "            root=data_root,\n",
        "            n=n_vis,\n",
        "            seed=seed,\n",
        "            figpath=figpath\n",
        "        )\n",
        "\n",
        "    return stems_test, results\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# 2) Entry point\n",
        "# ------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "  if bundles is None:\n",
        "    bundles, input_shape = build_bundles_only(\n",
        "          data_root=\"./dataset/combined\",\n",
        "          batch=5,\n",
        "          shuffle=6000,\n",
        "          seed=123,\n",
        "          train_frac=0.8,\n",
        "          val_frac=0.1,\n",
        "      )\n",
        "  stems_test, metrics = main_test_only(\n",
        "      data_root=\"./dataset/combined\",\n",
        "      ###--밑에는 이미 학습된 모델:\n",
        "      local_model_path='./model/weights(01.20.26)/local/local_unet_ep0050.weights.h5',  ##---if just using the as-trained models, put None instead\n",
        "      global_model_path='./model/weights(01.20.26)/global/global_minmax_ep0050.weights.h5',\n",
        "      stems_test=bundles[\"ids\"][\"test\"],\n",
        "      n_vis=50, #- visualize할 test sample 개수\n",
        "      seed=1,\n",
        "      do_visualize=True,\n",
        "      do_metrics=True,\n",
        "      max_items_for_metrics=None,\n",
        "      figpath='./model/out' ##--응력필드(target01/reconstructed) 이미지 save directory\n",
        "      ##--처음 출력으로 나오는 global이 예측한 mm (=[min_log, max_log]는 따로 figpath에 같이 텍스트파일로 저장했습니다)\n",
        "  )"
      ],
      "metadata": {
        "id": "isCro6T4ZnsW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}